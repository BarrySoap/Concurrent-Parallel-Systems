\documentclass[12pt, a4paper]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{float}
\usepackage[english]{babel}
\usepackage[top=1.0in, bottom=1.0in, left=1.0in, right=1.0in]{geometry}
\graphicspath{{Images/}}
\linespread{1.25}

\title{\vspace{-3cm}Concurrent \& Parallel Systems - Coursework 2 Report}

\author{Glenn Wilkie-Sullivan - 40208762}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
\noindent This report will comprehensively examine the approaches to parallelising a sequential JPEG compressor, taken from user kornelski on GitHub. These approaches will range from CPU-based parallelism to GPU frameworks and architectures such as CUDA and OpenCL. The ideal result is an optimised JPEG compressor which runs significantly faster with the new approaches implemented.
\end{abstract}

\section{Introduction and Background}
In order to understand how JPEG (Joint Photographic Experts Group) compression can be parallelised, we must first investigate the overall process and purpose of the compression. According to techradar.com,  JPEG compression, "is a lossy compression format conceived explicitly for making photo files smaller and exploits the imperfect characteristics of our perception". The process for this can be split into five main steps:

\begin{itemize}
\item Covert RGB colours of the image to YCbCr (Luminance, Chroma: Blue; Chroma: Red) colour space.
\item Preprocess image for DCT (Discrete Cosine Transformation) conversion.
\item DCT transformation.
\item Coefficient Quantization
\item Lossless Encoding
\end{itemize}

We will touch more on these concepts in the following sections, which involve evaluating the base program, suggesting a better solution for optimisation, implementing it as described, discussing the results and wrapping up the findings. First, the program must be examined and evaluated based on it's overall performance.

\section{Initial Analysis}
For this project, the JPEG compressor created by the user 'kornelski' on GitHub will be analysed. The link for this repository can be found as a reference. The specifications used to run this compressor and analyse it are as follows:

\begin{itemize}
\item CPU: Intel Core i7-6700HQ @ 2.60GHz (4 cores, 8 threads)
\item GPU: NVIDIA GeForce GTX 960M (4GB, GDDR5) \\
\end{itemize}

As such, the report will detail a methodology and results assuming these specifications. To start with, I used the Visual Studio 2017 diagnostic tools to analyse the overall CPU usage of the program when given various parameters. The program has a range of functionality, and the most pressing of them is the exhaustive test for the compressor, amongst the general compression and decompresion. First, we will look at the general compression algorithm, which has multiple variables affecting the execution time. Among these variables is the image size and quality factor. The quality factor is simply a number ranging from 0 - 100, relative to how sharp the resulting image should be. By modifying the quality factor and image size over 10 runs, the following results were found: \\

\begin{table}[H]
    \centering
    \begin{tabular}{| l | l | l | l | l | l |}
    \hline
    Image Size & Quality Factor: 40 & Quality Factor: 60 & Quality Factor: 80 & Quality Factor: 100  \\ \hline
    100 x 100 & 0.005 seconds & 0.006 seconds & 0.010 seconds & 0.010 seconds \\ \hline
    200 x 200 & 0.0089 seconds & 0.0213 seconds & 0.0129 seconds & 0.0177 seconds \\ \hline
    400 x 400 & 0.0337 seconds & 0.0339 seconds & 0.0369 seconds & 0.0445 seconds \\ \hline
    800 x 800 & 0.1309 seconds & 0.1343 seconds & 0.1492 seconds & 0.1797 seconds \\ \hline
    1600 x 1600 & 0.3461 seconds & 0.3586 seconds & 0.381 seconds & 0.4653 seconds \\ \hline
    3000 x 3000 & 1.0833 seconds & 1.0854 seconds & 1.1238 seconds & 1.2284 seconds \\ \hline
    \end{tabular}
    \caption{JPEG Compression Execution Time}
\end{table}

These findings were exlusively within Visual Studio, Release mode, x86. As we can see from table 1, the program itself is very fast. The quality factor of the compression doesn't seem to have much effect on the execution time - in most cases, there was only a rise of roughly 3.5\%. This table will eventually be used as a comparison after a parallelised solution is implemented - for now, we have to identify the bottleneck(s) of the program, so that we have a foundational understanding of where the program can be parallelised. \\



\section{Methodology}

\section{Results and Discussion}

\section{Conclusion}

\newpage

\bibliographystyle{apalike}
\bibliography{bib}{}
\nocite{*}

\end{document}