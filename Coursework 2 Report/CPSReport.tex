\documentclass[12pt, a4paper]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{float}
\usepackage[english]{babel}
\usepackage[top=1.0in, bottom=1.0in, left=1.0in, right=1.0in]{geometry}
\usepackage[bottom]{footmisc}
\graphicspath{{Images/}}
\linespread{1.25}

\title{\vspace{-3cm}Concurrent \& Parallel Systems - Coursework 2 Report}

\author{Glenn Wilkie-Sullivan - 40208762}

\date{\today}

\makeatletter
\newcommand{\mypm}{\mathbin{\mathpalette\@mypm\relax}}
\newcommand{\@mypm}[2]{\ooalign{%
  \raisebox{.1\height}{$#1+$}\cr
  \smash{\raisebox{-.6\height}{$#1-$}}\cr}}
\makeatother

\begin{document}
\maketitle

\begin{abstract}
\noindent This report will comprehensively examine the approaches to parallelising a sequential JPEG compressor, taken from user kornelski on GitHub. These approaches will range from CPU-based parallelism to GPU frameworks and architectures such as CUDA and OpenCL. The ideal result is an optimised JPEG compressor which runs significantly faster with the new approaches implemented.
\end{abstract}

\section{Introduction and Background}
In order to understand how JPEG (Joint Photographic Experts Group) compression can be parallelised, we must first investigate the overall process and purpose of the compression. According to techradar.com,  JPEG compression "is a lossy compression format conceived explicitly for making photo files smaller and exploits the imperfect characteristics of our perception". The process for this can be split into five main steps: 

\begin{itemize}
\item Covert RGB colours of the image to YCbCr (Luminance, Chroma: Blue; Chroma: Red) colour space.
\item Preprocess image for DCT (Discrete Cosine Transformation) conversion.
\item DCT conversion.
\item Coefficient Quantization
\item Lossless Encoding \\
\end{itemize}

Before analysis is commenced, we must establish an understanding of how these concepts work. In reference to the process list, we will analyse each step of the compression process. \\
In order to convert RGB colours to a YCbCr colour space, each component transform of the image provides an input value between 0 - 255 and converts it into Y, Cb, Cr values in the range 0 - 255, -128 - 127, and -128 - 127 respectively. The Y (luminance) component is then level-shifted down to the range -128 - 127, and the input tile of the level-shifted symmetric YCbCr colour space is used as the input for the next step of the compression process. \\
To preprocess for discrete cosine transformation, the image is first split up into equally sized blocks. The pixel matrix for each block is then centralised around 0, and 127 is subtracted from each value. This process looks as follows: \\

\begin{figure}[H]
	\centering
		\includegraphics[width=0.8\textwidth]{"dcttransform"}
		\caption{Discrete Cosine Transformation Preparation}
\end{figure}

After obtaining these values, the discrete cosine transform (DCT) helps separate the image into parts (or spectral sub-bands) of differing importance (with respect to the image's visual quality). After the image is split into equal blocks, the two-dimensional DCT is computed for each block. The DCT coefficients are then quantized, coded, and transmitted. The JPEG receiver (or JPEG file reader) decodes the quantized DCT coefficients, computes the inverse two-dimensional DCT of each block, and then puts the blocks back together into a single image. For typical images, many of the DCT coefficients have values close to zero. These coefficients can be discarded without seriously affecting the quality of the reconstructed image \textit{(MathWorks, pp. 7, 2018)}. \\
From here, any values left in the pixel matrix close to 0 are converted to 0, then divided by a matrix from the standard JPEG quantisation table (for luminance). \\
The final step is lossless encoding, using Huffman coding (which the analysed application utilises). This is to preserve the quality of the image while reducing the file size - by using a specific method for choosing the representation for each symbol, this results in a prefix-free code that expresses the most common characters using shorter strings of bits than are used for less common source symbols in the image chunks/parts/spectral sub-bands. No other mapping of individual source symbols to unique strings of bits will produce a smaller average output size when the actual symbol frequencies agree with those used to create the code.

We will reference these concepts in the following sections - which involve evaluating the base program, suggesting a better solution for optimisation, implementing it as described, discussing the results and wrapping up the findings. First, the program must be examined and evaluated based on its overall performance.

\section{Initial Analysis}

For this project, the JPEG compressor created by the user 'kornelski' on GitHub will be analysed. The link for this repository can be found as a reference. The specifications used to run this compressor and analyse it are as follows: \\

\begin{itemize}
\item CPU: Intel Core i7-6700HQ @ 2.60GHz (4 cores, 8 threads)
\item GPU: NVIDIA GeForce GTX 960M (4GB, GDDR5) \\
\end{itemize}

As such, the report will detail a methodology and results assuming these specifications. To start with, I used the Visual Studio 2017 diagnostic tools to analyse the overall CPU usage of the program when given various parameters. The program has a range of functionality, and the most pressing of them is the exhaustive test for the compressor, amongst the general compression and decompresion. For this report, we will look exclusively we at the general compression algorithm, which has multiple variables affecting the execution time. Among these variables is the image size and quality factor. The quality factor is simply a number ranging from 0 - 100, relative to how sharp the resulting image should be. By modifying the quality factor and image size over 20 runs, the following execution times were found: \\

\begin{table}[H]
    \centering
    \begin{tabular}{| l | l | l | l | l | l |}
    \hline
    Image Size & Quality Factor: 40 & Quality Factor: 60 & Quality Factor: 80 & Quality Factor: 100  \\ \hline
    100 x 100 & 0.005 seconds & 0.006 seconds & 0.010 seconds & 0.010 seconds \\ \hline
    200 x 200 & 0.0089 seconds & 0.0213 seconds & 0.0129 seconds & 0.0177 seconds \\ \hline
    400 x 400 & 0.0337 seconds & 0.0339 seconds & 0.0369 seconds & 0.0445 seconds \\ \hline
    800 x 800 & 0.1309 seconds & 0.1343 seconds & 0.1492 seconds & 0.1797 seconds \\ \hline
    1600 x 1600 & 0.3461 seconds & 0.3586 seconds & 0.381 seconds & 0.4653 seconds \\ \hline
    3000 x 3000 & 1.0833 seconds & 1.0854 seconds & 1.1238 seconds & 1.2284 seconds \\ \hline
    \end{tabular}
    \caption{JPEG Compression Execution Time}
\end{table}

These findings were exclusively within Visual Studio, Release mode, x86. As we can see from table 1, the program itself is very fast. The quality factor of the compression doesn't seem to have much effect on the execution time - in most cases, there was only a rise of roughly 3.5\%. When modifying the image size, the rise in execution time can range from between 160\% to 288\% $\mypm$ 10\% - in order to effectively test the execution time, the image size is the important variable to focus on. When using methods or parallelisation such as multi-threading, OpenMP (Open Multi-Processing) or GPU architectures, it seems likely that at lower image sizes, the initialisation of threads, parallelised loops or quantisation will have more strain on the CPU than the image compression itself. With that in mind, images with a pixel amount larger than 9,000,000 will be far more useful in testing the compression, and will be used as test cases in the following sections. Table 1 will eventually be used as a comparison after a parallelised solution is implemented - for now, the bottleneck(s) of the program must be identified, such that we have a foundational understanding of where the program can be parallelised. \\

The main hot-path of the program in main can be seen as follows:

\begin{figure}[H]
	\centering
		\includegraphics[width=0.6\textwidth]{"hotpath"}
		\caption{JPEG Compressor Hot-Path}
\end{figure}

From this, we can extract that the image loading and image comparison functions are costly; these load function simply load the image from a path with predetermined dimension values and parses it accordingly. The image comparison function computes the image error statistics, used to measure the accuracy of the observed image to the expected image. These functions cannot be parallelised, as the operations are inherently sequential. However, the general compression algorithms are pressing the CPU more than these functions; While figure 1 shows the CPU usage within main, we have to look further in the code to find the bottleneck of the program. The almost-full list of function CPU usage is: \\

\begin{table}[H]
    \centering
    \begin{tabular}{| l | l | l |}
    \hline
    Function & CPU Unit Usage & CPU Usage \%  \\ \hline
    jpge::compress\_image\_to\_jpeg\_file() & 3337 & 45.19\% \\ \hline
    jpge::compress\_image\_to\_stream() & 3334 & 45.15\% \\ \hline
    jpge::jpeg\_encoder::read\_image() & 1930 & 26.13\% \\ \hline
    do\_png() & 1710 & 23.16\% \\ \hline
    parse\_png\_file() & 1710 & 23.16\% \\ \hline
    stbi\_load() & 1720 & 23.16\% \\ \hline
    \end{tabular}
    \caption{Individual Function CPU Usage}
\end{table}

When looking at the definitions and calling of these functions, it appears as though there are multiple bottlenecks within the 'compress\_image\_to\_jpeg\_file' function. The CPU usage for this function is split between reading in the image, taking subsamples of the coordinate data and compressing the image subsequently. A handful of functions from table 2 point to these lines, and it seems logical to assume that this section will be parallelised for better performance using our previously mentioned techniques. When analysing the function itself, the line-by-line breakdown shows some potential areas of parallelisation: \\

\begin{figure}[H]
	\centering
		\includegraphics[width=0.99\textwidth]{"potentialareas"}
		\caption{Potential Spots of Parallelisation}
\end{figure}

As seen in figure 2, the 'quantize\_pixels' function is compressing a range of values related to the image into a single quantum value, such that with less discrete variables, the image should be easier to compress. These discrete values include: sample 'sub-images' which come from the main image, the discrete cosine transform and quantisation of the image dimensions, as well as the values returned from the Huffman\footnote{Huffman coding is a lossless data compression algorithm.}\footnote{Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data.} encoding function. When optimising this application, threaded approaches are generally useful for breaking down and parallelising looped processes, as are parellelised loops supported by OpenMP. As long as the discrete/quantum variables can be shared, the application should benefit greatly from these techniques in the overall speed of execution. \\

\section{Methodology}
As seen from the initial analysis, the main bottlenecks lie in the quantisation and image reading processes. There are numerous operations in the program which cannot be parallelised, such as the image I/O functions - as such, we will examine the approaches of parallelising the applicable sections of the program in this section. A selection of parallelisation techniques can be prepared to optimise the application - these techniques can be split into two types: CPU-based, and GPU-based. In this report, we will aim to look at CPU-based approaches such as multithreading, OpenMP parallelisation and MPI, as well as utilising GPU-based approaches such as the OpenCL and CUDA architectures. \\
The first method of optimisation should and will be general improvements around the application - for such a large program, it seems likely that it contains some superfluous code or functions which could be improved. If there are instances of slow code which can be quickly improved or removed, it will be changed. After this is done, the parallelisation techniques should be implemented and executed in order of difficulty. Starting off with the easiest - OpenMP will be used as a proof of concept that particular facets of the program can be parallelised and optimised. If applicable, the multiple occurences of OpenMP parallelisation should utilise the available integrated functions to make the process more robust. However, proving the compression process is optimised will be difficult - parallelisation can be proved by methods such as OpenMP's built-in functions or printing the thread numbers. However, while the application may be faster, the only way to test the compression is comparing the file sizes. In this report, the findings will reference the execution time of the application and the difference in file size. \\
Following this, a multithreaded approach will be implemented and tested, as well as a possible MPI solution. Theoretically, in the 'compress\_image' and 'read\_image' functions where a bottleneck exists, the task could be distributed between a number of threads relative to the CPU; As long as the resources are shared during the quantisation process, then the application is almost guaranteed to be faster. Similarly, if MPI is used, the in-built functions can distribute the parallelism among the tasks to improve performance. \\
In order to measure the speedup and efficiency, the execution time of each run must be serialised. For unbiased and high-coverage results, the program execution times in the results section will be an average of 20 runs of the application. The speedup and efficiency values will be presented in a table as such (assuming a quality factor of 100, given its relatively low effect on performance): \\

\begin{table}[H]
    \centering
    \begin{tabular}{| l | l | l | l | l |}
    \hline
    Image Size (Pixels) & Sequential Time & Parallel Time & Speedup & Efficiency\\ \hline
    x pixels, y pixels & w seconds & x seconds & y seconds & z seconds \\ \hline
    \end{tabular}
    \caption{Speedup and Efficiency Results}
\end{table}

Where x seconds is the time before implementation, and y seconds is the time after implementation. These tables may have more or less rows depending on the amount of parallelisation for that technique - the time before implementation will be taken from table 1. To visualise these results, R will be used to plot the average or overall time for each technique, for ease of analysis.

\section{Results and Discussion}
Before any results are presented - the following results were all found within Visual Studio 2017, Release Mode, x86 with the forementioned CPU/GPU specifications. Later in this section, an R script was used to plot the execution times of each technique against the respective image sizes, assuming a quality factor of 100 each time.
Firstly, for each technique used, the execution time was serialised into a CSV file, and the average time of 20 runs is taken as the estimate time of execution. As discussed before, a selection of techniques were used to parallelise this program - multithreading, OpenMP, OpenCL and CUDA.

\subsection{CPU-based Techniques}
Starting with the easiest implementation, OpenMP, this involved introducing parallel loops into the application. The bottlenecked areas lie around the read\_image and compress\_image functions. With this in mind, multiple parallelised loops were placed in these functions where iteration occurred - the following execution times and speedup was found:

\begin{table}[H]
    \centering
    \begin{tabular}{| l | l | l | l | l |}
    \hline
    Image Size (Pixels) & Sequential Time & Parallel Time & Speedup & Efficiency \\ \hline
    100 x 100 & 0.0039 seconds & 0.0501 seconds & 0.0778 & 1.0006 \\ \hline
    800 x 800 & 0.10755 seconds & 0.1815 seconds & 0.5926 & 0.99994 \\ \hline
    1600 x 1600 & 0.28455 seconds & 0.34975 seconds & 0.8136 & 0.99997 \\ \hline
    3000 x 3000 & 0.7453 seconds & 0.67925 seconds & 1.0972 & 1.00004 \\ \hline
    8000 x 8000 & 3.972 seconds & 3.1194 seconds & 1.2733 & 1.00002 \\ \hline
    \end{tabular}
    \caption{OpenMP Speedup and Efficiency Results}
\end{table}

As seen from the above table, the usefulness of OpenMP and it's reliability will only grow exponentially relative to the image size - i.e. as the image gets bigger, OpenMP parallelisation will make the compression process faster. However, at smaller image sizes, the initialisation of parallel loops is more costly than compressing the image sequentially. In the application, we can safely assume that the threshold of parallelised speedup lies at an image size of roughly 2000x2000 pixels. The inclusion of scheduling, whether it be static or dynamic, is completely superfluous - the mapping of threads to relative tasks in the loop cannot speedup the program due to the size of the task. Similarly, resources do not have to be shared amongst the costly functions. In some instances of the program, the variables have to/must be shared amongst loops, such that the output stays correct. However, on examination, this wasn't necessary for any speedup within the program. As a comparison, the multithreading example will be detailed.

As a reminder, in the multithreading example, a CPU with 8 threads will be used as the context of analysis. The implementation of multithreading within the application was very similar to the OpenMP method - for the amount of threads relative to the processor, run a given task among those threads. However, this technique had a significantly slower runtime, as seen in the following results:

\begin{table}[H]
    \centering
    \begin{tabular}{| l | l | l | l | l |}
    \hline
    Image Size (Pixels) & Sequential Time & Parallel Time & Speedup & Efficiency \\ \hline
    100 x 100 & 0.0039 seconds & 0.3627 seconds & 0.01075 & 1.00025 \\ \hline
    800 x 800 & 0.10755 seconds & 3.1934 seconds & 0.03368 & 0.099996 \\ \hline
    1600 x 1600 & 0.28455 seconds & 10.4485 seconds & 0.02723 & 1.00013 \\ \hline
    3000 x 3000 & 0.7453 seconds & 32.44785 seconds & 0.02297 & 0.99996 \\ \hline
    8000 x 8000 & 3.972 seconds & 223.42575 seconds & 0.01778 & 0.99987 \\ \hline
    \end{tabular}
    \caption{Multithreading Speedup and Efficiency Results}
\end{table}

The main issue around the threaded approach is the launching/initialisation of threads. As seen in the following figure:

\begin{figure}[H]
	\centering
		\includegraphics[width=0.99\textwidth]{"ThreadLaunch"}
		\caption{Thread Launching Bottleneck}
\end{figure}

The launching of a thread, even when compressing an image as small as 1600x1600, quickly becomes the most costly function in the application - using 31\% of the CPU resources. Assuming the functions utilising multithreading are being iterated a large amount of times, it makes sense why the launching of so many threads would bottleneck the program. The parallel time, just like the sequential time, seems to scale with the image size, almost exponentially. Even when using a mutex within the forementioned blocks of code, the application is still incredibly slow. Based on these results we can conclusively say that the launching of threads in this context is far more costly than initialising parallel loops within OpenMP. As there are two opposite types of results (both CPU-based), comparing them to another technique should show which of the two is the outlier. The technique to be analysed now will be CUDA, a GPU parallelisation technique.

\subsection{GPU-based Techniques}

\section{Conclusion}

\newpage

\bibliographystyle{abbrv}
\bibliography{bib}{}
\nocite{*}

\end{document}